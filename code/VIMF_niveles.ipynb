{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzwjeJHGAt18",
        "outputId": "58fd4c04-92a4-4bd9-a80d-ddf3811d6ded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Activar Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P98ztHt-AyZK"
      },
      "outputs": [],
      "source": [
        "# Realiza el composite de q, u, y v y calcula el VIMF para niveles específicos.\n",
        "\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import xarray as xr\n",
        "\n",
        "# Datos básicos.\n",
        "\n",
        "path = \"/content/drive/MyDrive/Colab/Onset_Mexico/\"\n",
        "path_e = path + \"data/Runmean/\"\n",
        "path_d = path + \"data/\"\n",
        "path_r = path + \"results/composite/\"\n",
        "path_v = path + \"results/VIMF/\"\n",
        "\n",
        "# Si no existe la carpeta, la crea.\n",
        "for i in [path_e, path_d, path_r, path_v]:\n",
        "    if not os.path.exists(i): os.mkdir(i)\n",
        "\n",
        "# Cantidad de días a promediar.\n",
        "d = 20\n",
        "\n",
        "# Variable, nivel de presión y región.\n",
        "vars = [ \"u\", \"v\", \"q\" ]\n",
        "vars_grib = [ \"var\" + x for x in np.arange(131, 134).astype(str) ]\n",
        "levels = [ \"1000\", \"975\", \"950\", \"925\", \"900\", \"875\", \"850\", \"825\",\n",
        "    \"800\", \"775\", \"750\", \"700\", \"650\", \"600\", \"550\", \"500\" ]\n",
        "\n",
        "# Años comprendidos en CHIRPS.\n",
        "years = list( range(1981, 2021) )\n",
        "\n",
        "# Cargamos la información de desfase para cada año.\n",
        "shift = []\n",
        "delta = []\n",
        "shift.append ( pd.read_csv(path_d + \"shift_CHIRPS.csv\",\n",
        "    index_col = \"Año\", squeeze = True) )\n",
        "shift.append( pd.read_csv(path_d + \"shift_withdrawal_CHIRPS.csv\",\n",
        "    index_col = \"Año\", squeeze = True) )\n",
        "\n",
        "# Fecha pivote del desfase. \n",
        "for i in [0, 1]:\n",
        "    delta.append( shift[i].max() )\n",
        "    shift[i] = delta[i] - shift[i]\n",
        "\n",
        "onset = [ \"onset\", \"withdrawal\" ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Idwuox8VCFXI",
        "outputId": "481c33f5-8e08-4c33-cc1a-56dd69195f8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing u:\n",
            "1000, 975, 950, 925, 900, 875, 850, 825, 800, 775, 750, 700, 650, 600, 550, 500, \n",
            "Processing v:\n",
            "1000, 975, 950, 925, 900, 875, 850, 825, 800, 775, 750, 700, 650, 600, 550, 500, \n",
            "Processing q:\n",
            "1000, 975, 950, 925, 900, 875, 850, 825, 800, 775, 750, 700, 650, 600, 550, 500, "
          ]
        }
      ],
      "source": [
        "# Realizamos el composite.\n",
        "\n",
        "# Todas las variables.\n",
        "for v in range(len(vars)):\n",
        "    print(f\"\\nProcessing {vars[v]}:\")\n",
        "\n",
        "    # Todos los niveles.\n",
        "    for n in range(len(levels)):\n",
        "        fname = ( \"onset_\" + vars[v] + \"_\" + levels[n] + \"_mean_20_dias.nc\" )\n",
        "        ds = xr.open_dataset( path_e + fname)\n",
        "        ds = ds.rename( {vars_grib[v]: vars[v]} )\n",
        "        # Onset y withdrawal, composite de la distribución espacial.\n",
        "        for j in range(len(onset)):\n",
        "            # Creamos un Dataset para cada año.\n",
        "            ds_year = []\n",
        "            for i in range( len(years) ):\n",
        "                ds_year.append( ds.sel( time = ds.time.dt.year.isin(\n",
        "                    years[i] ) ) )\n",
        "                # Convertimos las fechas a dayofyear.\n",
        "                ds_year[i] = ds_year[i].groupby(\n",
        "                    \"time.dayofyear\" ).mean( \"time\" )\n",
        "                # Coordenada para concatenar.\n",
        "                ds_year[i].assign_coords( {\"concat\": i} )\n",
        "                # Corregimos el dayofyear para los años bisiestos.\n",
        "                if years[i] % 4 == 0: ds_year[i][\"dayofyear\"] = range(1, 366)\n",
        "            # Alineamos las series de tiempo.\n",
        "            for i in range( len(ds_year) ):\n",
        "                ds_year[i] = ds_year[i].shift(\n",
        "                    { \"dayofyear\": shift[j].iloc[i] } )\n",
        "            # Concatenamos y promediamos por dayofyear.\n",
        "            composite = xr.concat(ds_year, dim = \"concat\")\n",
        "            composite = composite.mean(\"concat\")\n",
        "            if j == 1:\n",
        "                # Agregamos los primeros días del\n",
        "                # año después del último día del año.\n",
        "                x = composite.isel(dayofyear = slice(0, 60))\n",
        "                x[\"dayofyear\"] = np.arange(366, 366 + 60)\n",
        "                composite = xr.concat( (composite, x), dim = \"dayofyear\" )\n",
        "            # Centramos el índice a la fecha de onset.\n",
        "            composite = composite.sel( dayofyear = slice(\n",
        "                delta[j] - 60, delta[j] + 60 ) )\n",
        "            composite[\"dayofyear\"] = range(-60, 61)\n",
        "            # Establecemos un CRS.\n",
        "            # Guardamos el netcdf.\n",
        "            composite.to_netcdf( path_r + onset[j] + \"_composite_\" + vars[v]\n",
        "                + \"_\" + levels[n] + \"_mean_20_dias.nc\" )\n",
        "            \n",
        "        print(f\"{levels[n]},\", end = \" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zCwF_kie_ADY",
        "outputId": "95fda3b6-1c39-41cb-844f-7003de887291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000, 975, 950, 925, 900, 875, 850, 825, 800, 775, 750, 700, 650, 600, 550, 500, "
          ]
        }
      ],
      "source": [
        "# Unir viento.\n",
        "\n",
        "# Todos los niveles.\n",
        "for n in range(len(levels)):\n",
        "\n",
        "    # Viento zonal y meridional.\n",
        "    u = []\n",
        "    for j in range(len(onset)):\n",
        "        for v in [0, 1]:\n",
        "            fname = ( onset[j] + \"_composite_\" + vars[v]\n",
        "                + \"_\" + levels[n] + \"_mean_20_dias.nc\" )\n",
        "            u.append( xr.open_dataset( path_r + fname ) )\n",
        "\n",
        "        # Unimos en un solo netcdf.\n",
        "        ds = xr.merge( ( u[0], u[1] ) )\n",
        "\n",
        "        ds.to_netcdf( path_r + onset[j] + \"_wind_\"\n",
        "            + levels[n] + \"_mean_20_dias.nc\" )\n",
        "        \n",
        "    print(f\"{levels[n]},\", end = \" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "915oHIlaIQTC"
      },
      "outputs": [],
      "source": [
        "# Concatenamos todos los niveles de presión.\n",
        "\n",
        "ds = []\n",
        "a = [64, 96]\n",
        "b = [96, 128]\n",
        "names = [\"q\", \"wind\"]\n",
        "\n",
        "for i in range(len(names)):\n",
        "    for j in range(len(onset)):\n",
        "        files = os.listdir(path_r)[ (a[i] + j) : (b[i] + j) : 2]\n",
        "        files = [path_r + x for x in files]\n",
        "        ds = xr.open_mfdataset( files, combine = \"nested\",\n",
        "            concat_dim = \"plev\", parallel = True )\n",
        "        ds.to_netcdf( path_r + onset[j] + \"_composite_\"\n",
        "            + names[i] + \"_mean_20_dias.nc\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3qzxIrVrqbk"
      },
      "outputs": [],
      "source": [
        "# Cálculo de VIMF.\n",
        "\n",
        "for j in range(len(onset)):\n",
        "    wind = xr.open_dataset( path_r + onset[j]\n",
        "        + \"_composite_wind_mean_20_dias.nc\" )\n",
        "    q    = xr.open_dataset( path_r + onset[j]\n",
        "        + \"_composite_q_mean_20_dias.nc\"    )\n",
        "\n",
        "    # Límites de cálculo.\n",
        "    lim = (10e4, 7e4)\n",
        "\n",
        "    # Preparamos las variables.\n",
        "    q[\"u\"] = q[\"q\"]\n",
        "    q = q.rename(q = \"v\")\n",
        "    mf = q * wind\n",
        "\n",
        "    # Divergencia.\n",
        "    mf = mf.rename(u = \"qu\", v = \"qv\")\n",
        "    mf[\"dqu_dx\"] = ( (\"dayofyear\", \"plev\", \"lat\", \"lon\"),\n",
        "        np.gradient( mf[\"qu\"], axis = 2 ) )\n",
        "    mf[\"dqv_dx\"] = ( (\"dayofyear\", \"plev\", \"lat\", \"lon\"),\n",
        "        np.gradient( mf[\"qv\"], axis = 2 ) )\n",
        "\n",
        "    #mf = mf.drop([\"qu\", \"qv\"])\n",
        "    mf = mf.sel( plev = slice( int(10e4), int(7e4) ) )\n",
        "\n",
        "    mf[\"vidmf\"] = mf[\"dqu_dx\"] + mf[\"dqv_dx\"]\n",
        "    #mf = mf.drop([\"dqu_dx\", \"dqv_dx\"])\n",
        "\n",
        "    # Integral vertical.\n",
        "    vidmf = mf.drop([\"qu\", \"qv\", \"dqu_dx\", \"dqv_dx\"]).mean(\"plev\").copy()\n",
        "    vimf = mf.drop([\"dqu_dx\", \"dqv_dx\", \"vidmf\"]).mean(\"plev\").copy()\n",
        "\n",
        "    vimf[\"qu\"] = ( (\"dayofyear\", \"lat\", \"lon\"),\n",
        "        np.trapz( mf[\"qu\"], mf[\"plev\"], axis = 1) )\n",
        "    vimf[\"qv\"] = ( (\"dayofyear\", \"lat\", \"lon\"),\n",
        "        np.trapz( mf[\"qv\"], mf[\"plev\"], axis = 1) )\n",
        "    vidmf[\"vidmf\"] = ( (\"dayofyear\", \"lat\", \"lon\"),\n",
        "        np.trapz( mf[\"vidmf\"], mf[\"plev\"], axis = 1) )\n",
        "\n",
        "    vimf.to_netcdf(path_v +  onset[j] + \"_composite_vimf_\"\n",
        "        + f\"{lim[0]/100:.0f}\" + \"_\" + f\"{lim[1]/100:.0f}\" + \"_mean_20_dias.nc\")\n",
        "    vidmf.to_netcdf(path_v +  onset[j] + \"_composite_vidmf_\"\n",
        "        + f\"{lim[0]/100:.0f}\" + \"_\" + f\"{lim[1]/100:.0f}\" + \"_mean_20_dias.nc\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPqAz1LWArldCJwPPNyhlsH",
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.9 ('gv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "cce8d45697d85db6636183b0f945e1ffea3934773870cd67d93bb4bbe05deaec"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
